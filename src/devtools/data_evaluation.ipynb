{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for PydanticAI to work with Jupyter (nested event loops)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=918853;https://logfire.pydantic.dev/xmandeng/pastel\u001b\\\u001b[4;36mhttps://logfire.pydantic.dev/xmandeng/pastel\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "from pastel.classifiers import classify_insurance_image\n",
    "from pastel.parsers import parse_assertion, parse_evidence, check_grammar\n",
    "from pastel.evaluation import consolidate_evaluations\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import pandas as pd\n",
    "import logfire\n",
    "import os\n",
    "from pastel.helpers import (\n",
    "    load_images_from_directory,\n",
    "    create_consolidated_image,\n",
    "    export_evaluation_to_markdown,\n",
    ")\n",
    "from pastel.models import (\n",
    "    InsightPlots,\n",
    "    InputModel,\n",
    ")\n",
    "\n",
    "if os.getenv(\"PYDANTIC_LOGFIRE_TOKEN\"):\n",
    "    logfire.configure(token=os.getenv(\"PYDANTIC_LOGFIRE_TOKEN\"))\n",
    "    logfire.instrument_openai()\n",
    "    logfire.instrument_anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Selection\n",
    "Select `index` value from 0 to 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from 0 to 14\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight Validation\n",
    "- Load datasets\n",
    "- Collect images\n",
    "- Parse Insight\n",
    "  - conclusion\n",
    "  - supporting premises\n",
    "- Evaluate premises\n",
    "- Check grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:49:12.362 Chat Completion with 'gpt-4o-mini' [LLM]\n",
      "15:49:13.487 Chat Completion with 'gpt-4o-mini' [LLM]\n",
      "15:49:15.127 Message with 'claude-3-7-sonnet-latest' [LLM]\n",
      "15:49:28.842 Message with 'claude-3-5-sonnet-latest' [LLM]\n",
      "15:49:30.072 Message with 'claude-3-7-sonnet-latest' [LLM]\n"
     ]
    }
   ],
   "source": [
    "insights_df = pd.read_excel(\"../data/Insights.xlsx\")\n",
    "\n",
    "input_model = InputModel(\n",
    "    name=insights_df[\"programname\"][index],\n",
    "    insight=insights_df[\"insight\"][index],\n",
    "    line_of_business=insights_df[\"line_of_business\"][index],\n",
    ")\n",
    "\n",
    "assertion = await parse_assertion(input_model)\n",
    "insight = await parse_evidence(assertion)\n",
    "\n",
    "# Retrieve data sets\n",
    "lrs = pd.read_excel(\"../data/lrs.xlsx\")\n",
    "lrs_data = lrs.loc[lrs[\"programname\"] == insight.name].drop(columns=[\"programname\"])\n",
    "\n",
    "images = InsightPlots(plots=load_images_from_directory(f\"../data/{insight.name.replace('/', '-')}\"))\n",
    "consolidated_image = create_consolidated_image(lrs_data, images, insight)\n",
    "premises = await classify_insurance_image(consolidated_image, insight.evidence)\n",
    "grammar = await check_grammar(insight)\n",
    "final_evaluation = await consolidate_evaluations(insight, premises, grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Validation Resuls\n",
    "- Display report\n",
    "- Write markdown to `../reports/` folder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Overall Assessment\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### \"This is a poorly performing auto liability program (EULR > 0.7) with stable pricing and similar performance for frequency and severity of claims across underwriting years.\"<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The insight is generally valid, though with some qualifications. The core claim about this being a poorly performing auto liability program with EULR > 0.7 is strongly supported by data showing values of 0.99, 0.86, and 0.76 across years. The claim about stable pricing is partially true - while there is baseline consistency, there are notable periodic adjustments. The claim about similar performance for frequency of claims is fully supported, with almost identical trajectories across policy years. The claim about similar severity performance is partially true - there are similarities but with more significant variations, especially after day 700. There are no grammatical errors in the insight. Overall, the main assertions are either true or partially true, with no false claims, making the insight valid with minor caveats about the degree of pricing stability and severity performance similarity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Conclusion\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### \"This is a poorly performing auto liability program\"<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Premises\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 1. The Expected Ultimate Loss Ratio (EULR) is greater than 0.7."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Status** <br>True <br>High confidence"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rationale** <br>The LRS Data table clearly shows the 'uf' column which appears to represent the Ultimate Loss Ratio. For 2021, the value is 0.99, for 2022 it's 0.86, and for 2023 it's 0.76. All of these values are greater than 0.7. Even though 2024 shows 'nan' (likely because it's incomplete data), the historical data strongly supports that the EULR is consistently above 0.7.<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 2. The program has stable pricing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Status** <br>Partially True <br>Medium confidence"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rationale** <br>The pricing chart (titled 'Written Premium per Risk and Days Covered') shows some fluctuations over time. While there is a baseline consistency with the 360 Rolling Median hovering around 200-300 range, there are notable spikes throughout the timeline, particularly around the Treaty markers. These spikes indicate periodic pricing adjustments rather than complete stability. The pricing appears to remain within a general range but with regular adjustments, suggesting partial stability with planned periodic changes.<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 3. There is similar performance for frequency of claims."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Status** <br>True <br>High confidence"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rationale** <br>The frequency chart ('CLAIM FREQUENCY PER POLICY') shows very similar patterns across different policy years (indicated by the colored lines 1-4). All lines follow almost identical trajectories, starting at 0 and increasing steadily over time toward a plateau of approximately 1.5 frequency per policy. The lines largely overlap, especially in the earlier periods, indicating consistent claim frequency performance across different policy years.<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 4. There is similar performance for severity of claims."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Status** <br>Partially True <br>Medium confidence"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rationale** <br>The severity chart ('LARGE CLAIMS/TOTAL CLAIMS') shows some similarity but with more variation than the frequency chart. There are two main trend lines (likely representing different claim size brackets: >50K and >100K). While the overall pattern is similar across years, there are more noticeable divergences, particularly after day 700 where the lines separate more distinctly. The general trend is comparable, but the variations are significant enough that it can only be considered partially similar in performance.<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Grammar\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### No grammatical errors found"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"---\"))\n",
    "display(Markdown(\"# Overall Assessment\\n\\n---\"))\n",
    "display(Markdown(f'#### \"{insight.insight}\"<br>'))\n",
    "display(Markdown(f\"## {final_evaluation.overall_valid}\"))\n",
    "display(Markdown(final_evaluation.reasoning))\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"## Conclusion\\n\\n\"))\n",
    "display(Markdown(f'#### \"{insight.conclusion}\"<br><br>'))\n",
    "\n",
    "display(Markdown(f\"---\"))\n",
    "display(Markdown(\"## Premises\\n\\n\"))\n",
    "\n",
    "if not premises:\n",
    "    display(Markdown(\"####No premises found\"))\n",
    "\n",
    "else:\n",
    "    for i, premise in enumerate(premises):\n",
    "        display(Markdown(f\"#### {i+1}. {premise.claim}\"))\n",
    "        display(Markdown(f\"**Status** <br>{premise.status} <br>{premise.confidence} confidence\"))\n",
    "        display(Markdown(f\"**Rationale** <br>{premise.reasoning}<br><br>\"))\n",
    "\n",
    "display(Markdown(f\"---\"))\n",
    "if grammar.errors:\n",
    "    error_list = \"\\n\".join([f\"- {error}\" for error in grammar.errors])\n",
    "    display(Markdown(\"## Grammar\\n\\n\" + error_list))\n",
    "else:\n",
    "    display(Markdown(\"## Grammar\\n\\n\"))\n",
    "    display(Markdown(\"#### No grammatical errors found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Markdown report saved to: ../reports/evaluation_pdl6muw8mJl9DL7bVO40nFOroodOnSFBG5e7zw+nAW32k7BiKehq6oLHwyItBjfw.md**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "md_result = export_evaluation_to_markdown(insight, final_evaluation, premises, grammar)\n",
    "display(Markdown(f\"**{md_result}**\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
