{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Selection\n",
    "Select `index` value from 0 to 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the index manually if needed\n",
    "# index = int(input(\"Enter an index value (0-14): \"))\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for PydanticAI to work with Jupyter (nested event loops)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=7193;https://logfire.pydantic.dev/xmandeng/verifyai\u001b\\\u001b[4;36mhttps://logfire.pydantic.dev/xmandeng/verifyai\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "from verifyai.classifiers import classify_insurance_image\n",
    "from verifyai.parsers import parse_assertion, parse_evidence, check_grammar\n",
    "from verifyai.evaluation import consolidate_evaluations\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import pandas as pd\n",
    "import logfire\n",
    "import os\n",
    "from verifyai.helpers import (\n",
    "    load_images_from_directory,\n",
    "    create_consolidated_image,\n",
    "    export_evaluation_to_markdown,\n",
    ")\n",
    "from verifyai.models import (\n",
    "    InsightPlots,\n",
    "    InputModel,\n",
    ")\n",
    "\n",
    "if os.getenv(\"PYDANTIC_LOGFIRE_TOKEN\"):\n",
    "    logfire.configure(token=os.getenv(\"PYDANTIC_LOGFIRE_TOKEN\"))\n",
    "    logfire.instrument_openai()\n",
    "    logfire.instrument_anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight Validation\n",
    "- Load datasets\n",
    "- Collect images\n",
    "- Parse Insight\n",
    "  - conclusion\n",
    "  - supporting premises\n",
    "- Evaluate premises\n",
    "- Check grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:19:07.642 Chat Completion with 'gpt-4o-mini' [LLM]\n",
      "02:19:09.851 Chat Completion with 'gpt-4o-mini' [LLM]\n",
      "02:19:13.883 Message with 'claude-3-7-sonnet-latest' [LLM]\n",
      "02:19:32.369 Message with 'claude-3-5-sonnet-latest' [LLM]\n",
      "02:19:35.525 Message with 'claude-3-7-sonnet-latest' [LLM]\n"
     ]
    }
   ],
   "source": [
    "insights_df = pd.read_excel(\"../data/Insights.xlsx\")\n",
    "\n",
    "input_model = InputModel(\n",
    "    name=insights_df[\"programname\"][index],\n",
    "    insight=insights_df[\"insight\"][index],\n",
    "    line_of_business=insights_df[\"line_of_business\"][index],\n",
    ")\n",
    "\n",
    "assertion = await parse_assertion(input_model)\n",
    "insight = await parse_evidence(assertion)\n",
    "\n",
    "# Retrieve data sets\n",
    "lrs = pd.read_excel(\"../data/lrs.xlsx\")\n",
    "lrs_data = lrs.loc[lrs[\"programname\"] == insight.name].drop(columns=[\"programname\"])\n",
    "\n",
    "images = InsightPlots(plots=load_images_from_directory(f\"../data/{insight.name.replace('/', '-')}\"))\n",
    "consolidated_image = create_consolidated_image(lrs_data, images, insight)\n",
    "premises = await classify_insurance_image(consolidated_image, insight.evidence)\n",
    "grammar = await check_grammar(insight)\n",
    "final_evaluation = await consolidate_evaluations(insight, premises, grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Validation Resuls\n",
    "- Display report\n",
    "- Write markdown to `../reports/` folder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Overall Assessment\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### \"This is a poorly performing auto liability program (EULR > 0.7) with stable pricing and similar performance for frequency and severity of claims across underwriting years.\"<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The insight contains significant inaccuracies. While it correctly identifies high EULR values (>0.7) and similar claim frequency performance across years, it incorrectly characterizes the program as having \"stable pricing\" when the evidence shows considerable volatility in pricing with significant fluctuations and spikes. Additionally, the claim about similar severity performance is only partially true, as there are notable divergences in severity patterns between different years, especially after day 700. These factual errors undermine the overall validity of the insight despite some accurate observations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Conclusion\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### \"This is a poorly performing auto liability program\"<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Premises\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 1. The Expected Ultimate Loss Ratio (EULR) is greater than 0.7."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Status** <br>Partially True <br>Medium confidence"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rationale** <br>From the LRS Data table at the top of the image, we can see the 'ulf' (which appears to be Ultimate Loss Factor or Ultimate Loss Ratio) values for multiple years. For 2021, the ulf is 0.99, for 2022 it's 0.86, and for 2023 it's 0.76. All of these values are indeed greater than 0.7. However, the 2024 value is marked as 'nan' (not a number), indicating no data is available yet. Since the claim doesn't specify a time period and one year has missing data, I've marked this as partially true rather than completely true.<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 2. The program has stable pricing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Status** <br>False <br>High confidence"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rationale** <br>The 'pricing.png' chart shows 'Written Premium per Risk and Days Covered' over time with a 360 Rolling Median. The chart displays significant fluctuations in pricing throughout the shown period, with multiple noticeable spikes (especially around Treaty 0, Treaty 2, and Treaty 3 markers). The premium values range from approximately 100 to over 500, showing considerable volatility rather than stability. There are periods of relative stability between the spikes, but the overall pattern demonstrates that pricing is not stable across the program's lifetime.<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 3. There is similar performance for frequency of claims."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Status** <br>True <br>High confidence"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rationale** <br>The 'frequency.png' chart shows claim frequency per policy over days. The lines for different years (color-coded as 1, 2, 3, and 4 in the legend, which appear to correspond to years 2021-2024) follow very similar trajectories. All lines start near zero and increase in a similar pattern, eventually plateauing around 1.4-1.5 frequency. While there are minor variations between the years, the overall pattern and trend of claim frequency performance is remarkably consistent across the observed periods, supporting the statement about similar performance.<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 4. There is similar performance for severity of claims."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Status** <br>Partially True <br>Medium confidence"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Rationale** <br>The 'severity.png' chart shows large claims/total claims data. There are visible differences in severity patterns between different years (represented by colored lines 1-4). While all lines show an upward trend over time, there are notable divergences, especially after around day 700 where one line (likely representing 2021) continues climbing more steeply while another line (likely 2022) plateaus at a lower level. The early performance (first ~350 days) shows more similarity between years with some volatility. Given these mixed patterns - similar directional trends but different magnitudes - the statement is partially true.<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Grammar\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### No grammatical errors found"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"---\"))\n",
    "display(Markdown(\"# Overall Assessment\\n\\n---\"))\n",
    "display(Markdown(f'#### \"{insight.insight}\"<br>'))\n",
    "display(Markdown(f\"## {final_evaluation.overall_valid}\"))\n",
    "display(Markdown(final_evaluation.reasoning))\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"## Conclusion\\n\\n\"))\n",
    "display(Markdown(f'#### \"{insight.conclusion}\"<br><br>'))\n",
    "\n",
    "display(Markdown(f\"---\"))\n",
    "display(Markdown(\"## Premises\\n\\n\"))\n",
    "\n",
    "if not premises:\n",
    "    display(Markdown(\"####No premises found\"))\n",
    "\n",
    "else:\n",
    "    for i, premise in enumerate(premises):\n",
    "        display(Markdown(f\"#### {i+1}. {premise.claim}\"))\n",
    "        display(Markdown(f\"**Status** <br>{premise.status} <br>{premise.confidence} confidence\"))\n",
    "        display(Markdown(f\"**Rationale** <br>{premise.reasoning}<br><br>\"))\n",
    "\n",
    "display(Markdown(f\"---\"))\n",
    "if grammar.errors:\n",
    "    error_list = \"\\n\".join([f\"- {error}\" for error in grammar.errors])\n",
    "    display(Markdown(\"## Grammar\\n\\n\" + error_list))\n",
    "else:\n",
    "    display(Markdown(\"## Grammar\\n\\n\"))\n",
    "    display(Markdown(\"#### No grammatical errors found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Markdown report saved to: ../reports/evaluation_pdl6muw8mJl9DL7bVO40nFOroodOnSFBG5e7zw+nAW32k7BiKehq6oLHwyItBjfw.md**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Save to folder\n",
    "md_result = export_evaluation_to_markdown(insight, final_evaluation, premises, grammar)\n",
    "display(Markdown(f\"**{md_result}**\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
